# -*- coding: utf-8 -*-
"""House Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WFyt6e6IPWyeNFpYZa4nbe753PgAMNHk
"""

# Install XGBoost
!pip install xgboost

# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

from xgboost import XGBRegressor

# Step 1: Create synthetic dataset
np.random.seed(42)
n_samples = 1000

data = {
    'LotArea': np.random.randint(500, 20000, n_samples),
    'YearBuilt': np.random.randint(1950, 2023, n_samples),
    'YearRemodAdd': np.random.randint(1950, 2023, n_samples),
    'TotalBsmtSF': np.random.randint(0, 3000, n_samples),
    'GrLivArea': np.random.randint(400, 4000, n_samples),
    'GarageCars': np.random.randint(0, 4, n_samples),
    'GarageArea': np.random.randint(0, 1000, n_samples),
    '1stFlrSF': np.random.randint(400, 3000, n_samples),
    '2ndFlrSF': np.random.randint(0, 2000, n_samples),
    'BsmtFullBath': np.random.randint(0, 3, n_samples),
    'FullBath': np.random.randint(1, 4, n_samples),
    'HalfBath': np.random.randint(0, 2, n_samples),
    'BedroomAbvGr': np.random.randint(1, 6, n_samples),
    'TotRmsAbvGrd': np.random.randint(3, 12, n_samples),
    'Fireplaces': np.random.randint(0, 3, n_samples),
    'OverallQual': np.random.randint(1, 11, n_samples),
    'OverallCond': np.random.randint(1, 11, n_samples),
    'KitchenQual': np.random.choice(['Ex', 'Gd', 'TA', 'Fa'], n_samples),
    'Neighborhood': np.random.choice(['CollgCr', 'Veenker', 'Crawfor', 'NoRidge', 'Mitchel', 'Somerst', 'NridgHt'], n_samples),
    'HouseStyle': np.random.choice(['1Story', '2Story', '1.5Fin', 'SLvl', 'SFoyer'], n_samples),
    'BldgType': np.random.choice(['1Fam', '2fmCon', 'Duplex', 'TwnhsE', 'Twnhs'], n_samples),
    'SaleCondition': np.random.choice(['Normal', 'Abnorml', 'Partial', 'AdjLand', 'Alloca'], n_samples)
}

# Step 2: Create DataFrame
df = pd.DataFrame(data)

# Generate target variable (SalePrice) with added weights and noise
df['SalePrice'] = (
    df['LotArea'] * 0.4 +
    df['GrLivArea'] * 1.5 +
    df['GarageArea'] * 1.3 +
    df['1stFlrSF'] * 1.1 +
    df['2ndFlrSF'] * 1.0 +
    df['FullBath'] * 3000 +
    df['HalfBath'] * 1500 +
    df['BedroomAbvGr'] * 1000 +
    df['Fireplaces'] * 4000 +
    df['OverallQual'] * 6000 +
    df['OverallCond'] * 1500 +
    np.random.normal(15000, 30000, n_samples)
).astype(int)

# Step 3: Feature/Target Split
X = df.drop('SalePrice', axis=1)
y = df['SalePrice']

# Step 4: Identify numerical and categorical columns
num_cols = X.select_dtypes(include=['int64', 'float64']).columns
cat_cols = X.select_dtypes(include=['object']).columns

# Step 5: Preprocessing pipelines
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

full_pipeline = ColumnTransformer([
    ('num', num_pipeline, num_cols),
    ('cat', cat_pipeline, cat_cols)
])

# Step 6: Apply transformations
X_processed = full_pipeline.fit_transform(X)

# Step 7: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Step 8: Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Linear Regression:")
print("R2 Score:", r2_score(y_test, y_pred_lr))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))

# Step 9: XGBoost Regressor
xgb_model = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

print("\nXGBoost Regression:")
print("R2 Score:", r2_score(y_test, y_pred_xgb))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))

# Step 10: Enhanced Visualization
plt.figure(figsize=(18, 10))

# 1. Predicted vs Actual - Linear Regression
plt.subplot(2, 3, 1)
sns.scatterplot(x=y_test, y=y_pred_lr, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title("Linear Regression: Actual vs Predicted")
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")

# 2. Predicted vs Actual - XGBoost
plt.subplot(2, 3, 2)
sns.scatterplot(x=y_test, y=y_pred_xgb, color='green')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title("XGBoost: Actual vs Predicted")
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")

# 3. Residuals Plot - Linear Regression
plt.subplot(2, 3, 3)
residuals_lr = y_test - y_pred_lr
sns.histplot(residuals_lr, bins=30, kde=True, color='blue')
plt.title("Linear Regression: Residual Distribution")
plt.xlabel("Residuals")
plt.ylabel("Frequency")

# 4. Residuals Plot - XGBoost
plt.subplot(2, 3, 4)
residuals_xgb = y_test - y_pred_xgb
sns.histplot(residuals_xgb, bins=30, kde=True, color='green')
plt.title("XGBoost: Residual Distribution")
plt.xlabel("Residuals")
plt.ylabel("Frequency")

# 5. Error Scatter Plot - Linear Regression
plt.subplot(2, 3, 5)
sns.scatterplot(x=y_pred_lr, y=residuals_lr, color='blue')
plt.axhline(0, color='r', linestyle='--')
plt.title("Linear Regression: Prediction Error")
plt.xlabel("Predicted SalePrice")
plt.ylabel("Residuals (Error)")

# 6. Error Scatter Plot - XGBoost
plt.subplot(2, 3, 6)
sns.scatterplot(x=y_pred_xgb, y=residuals_xgb, color='green')
plt.axhline(0, color='r', linestyle='--')
plt.title("XGBoost: Prediction Error")
plt.xlabel("Predicted SalePrice")
plt.ylabel("Residuals (Error)")

plt.tight_layout()
plt.show()

# Step 11: Feature Importance (XGBoost)
plt.figure(figsize=(12, 6))
xgb_importances = xgb_model.feature_importances_

# Convert one-hot encoded feature names
feature_names = full_pipeline.get_feature_names_out()

# Create dataframe for visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': xgb_importances
}).sort_values(by='Importance', ascending=False).head(20)  # Top 20 features

sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
plt.title("Top 20 XGBoost Feature Importances")
plt.xlabel("Importance Score")
plt.ylabel("Feature Name")
plt.tight_layout()
plt.show()